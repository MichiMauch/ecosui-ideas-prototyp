# Technische Dokumentation â€“ Content-Ideen-Generator

*Dokumentation fÃ¼r Entwickler, IT-Entscheider und CTOs*

---

## 1. Architektur-Ãœbersicht

### Stack

| Komponente | Technologie |
|------------|-------------|
| Laufzeit | Python 3.12 |
| Web-Framework | Streamlit (Single-Page-App) |
| KI-Modell | OpenAI GPT-5.2 (`gpt-5.2`) via OpenAI Python SDK |
| Analytics | Google Analytics Data API v1 Beta |
| Search | Google Search Console API v3 (webmasters) |
| Authentifizierung (Google) | Service Account (JSON-Key) |
| RSS-Parsing | feedparser |
| Web-Crawling | requests + stdlib `html.parser` |
| PDF-Export | fpdf2 (Pure-Python, keine SystemabhÃ¤ngigkeiten) |
| Persistenz | Lokale JSON-Datei (`data/ideas_history.json`) |
| Umgebungsvariablen | python-dotenv (`.env`-Datei oder Streamlit Secrets) |

### Deployment
Das System ist als Streamlit-App konzipiert und kann lokal (`streamlit run app.py`) oder Ã¼ber Streamlit Community Cloud betrieben werden. FÃ¼r Streamlit Cloud werden Geheimnisse Ã¼ber `st.secrets` injiziert und automatisch in `os.environ` Ã¼berfÃ¼hrt.

Keine Datenbank, kein externer State-Store â€“ alle Laufzeitdaten liegen im Streamlit Session State bzw. im lokalen Dateisystem.

---

## 2. Projektstruktur

```
content-idea-generator/
â”‚
â”œâ”€â”€ app.py                          # Streamlit UI â€“ Einstiegspunkt
â”œâ”€â”€ pipeline.py                     # Ideen-Pipeline-Orchestrator (4 Agenten)
â”œâ”€â”€ content_pipeline.py             # Artikel-Pipeline-Orchestrator (5 Agenten)
â”œâ”€â”€ evaluation_pipeline.py          # Bewertungs-Pipeline-Orchestrator (2 Agenten)
â”œâ”€â”€ export.py                       # Artikel-Export: Markdown + PDF (fpdf2)
â”œâ”€â”€ config.py                       # Zentrale Konfiguration (Konstanten, Feeds)
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ analyst.py                  # Agent 1: GA4 + GSC Analyse
â”‚   â”œâ”€â”€ trend_scout.py              # Agent 2: RSS-Feed-Analyse
â”‚   â”œâ”€â”€ strategist.py               # Agent 3: Ideen-Konzeption
â”‚   â”œâ”€â”€ editor.py                   # Agent 4: Ideen-Verfeinerung + JSON-Output
â”‚   â”œâ”€â”€ researcher.py               # Agent 5: Faktenrecherche fÃ¼r Artikel
â”‚   â”œâ”€â”€ writer.py                   # Agent 6: Artikel-Erstellung
â”‚   â”œâ”€â”€ fact_checker.py             # Agent 7: FaktenprÃ¼fung
â”‚   â”œâ”€â”€ evaluator.py                # Agent 8: QualitÃ¤tsbewertung
â”‚   â”œâ”€â”€ social_writer.py            # Agent 9: Social-Media-Texte
â”‚   â”œâ”€â”€ idea_context.py             # Agent 10: Kontext-Suche (Ideen-PrÃ¼fung)
â”‚   â””â”€â”€ idea_evaluator.py           # Agent 11: Ideen-Bewertung (Ideen-PrÃ¼fung)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ google_analytics.py         # GA4 API-Client
â”‚   â”œâ”€â”€ search_console.py           # GSC API-Client
â”‚   â”œâ”€â”€ rss_reader.py               # RSS-Feed-Fetcher
â”‚   â”œâ”€â”€ content_crawler.py          # Website-Crawler
â”‚   â”œâ”€â”€ google_trends.py            # Google Trends-Client (pytrends)
â”‚   â””â”€â”€ ideas_history.json          # Persistierte Ideen-Runs (auto-created)
â”‚
â”œâ”€â”€ .env                            # Lokale Umgebungsvariablen (nicht committed)
â”œâ”€â”€ requirements.txt                # Python-AbhÃ¤ngigkeiten
â””â”€â”€ credentials.json                # Google Service Account Key (nicht committed)
```

---

## 3. Datenquellen & APIs

### 3.1 Google Analytics 4 (`data/google_analytics.py`)

**Funktion:** `fetch_top_pages(property_id, credentials_file, days_back, limit)`

| Parameter | Typ | Standard | Beschreibung |
|-----------|-----|---------|--------------|
| `property_id` | `str` | â€“ | GA4-Property-ID (z.B. `"123456789"`) |
| `credentials_file` | `str` | â€“ | Pfad zur Service-Account-JSON-Datei |
| `days_back` | `int` | `7` | Lookback-Zeitraum in Tagen |
| `limit` | `int` | `20` | Maximale Anzahl Ergebniszeilen |

**GA4-Dimensionen:** `pageTitle`, `pagePath`
**GA4-Metriken:** `screenPageViews`, `engagementRate`
**Sortierung:** `screenPageViews` absteigend

**Return-Format:** `list[dict]`
```python
[
    {
        "page_title": str,        # Seitentitel
        "page_path": str,         # URL-Pfad (z.B. "/artikel/ezb-zinsen")
        "page_views": int,        # Anzahl Seitenaufrufe
        "engagement_rate": float, # Engagement-Rate in % (0â€“100)
    },
    ...
]
```

**Authentifizierung:** Service Account via `google-auth` Library. UnterstÃ¼tzt JSON-Inhalt direkt in `GOOGLE_CREDENTIALS_JSON` (Env-Var) oder als Dateipfad in `GOOGLE_CREDENTIALS_FILE`. Eingebaute Behandlung von Newlines in Private-Key-Strings.

---

### 3.2 Google Search Console (`data/search_console.py`)

**Funktion 1:** `fetch_top_queries(site_url, credentials_file, days_back, limit)`

| Parameter | Typ | Standard | Beschreibung |
|-----------|-----|---------|--------------|
| `site_url` | `str` | â€“ | GSC-Site-URL (z.B. `"https://example.com/"`) |
| `credentials_file` | `str` | â€“ | Pfad zur Service-Account-JSON-Datei |
| `days_back` | `int` | `7` | Lookback-Zeitraum in Tagen |
| `limit` | `int` | `25` | Maximale Anzahl Ergebniszeilen |

**Dimension:** `query` | **Sortierung:** `impressions` absteigend

**Return-Format:** `list[dict]`
```python
[
    {
        "query": str,        # Suchanfrage
        "impressions": int,  # Anzahl Impressionen
        "clicks": int,       # Anzahl Klicks
        "ctr": float,        # Click-Through-Rate in % (gerundet auf 2 Stellen)
        "position": float,   # Ã˜ Google-Position (gerundet auf 1 Stelle)
    },
    ...
]
```

---

**Funktion 2:** `fetch_top_pages_by_position(site_url, credentials_file, days_back, limit, min_position, max_position)`

Identifiziert seitenbasiert die Google-Positionen. PrimÃ¤r genutzt fÃ¼r Fast-Ranker-Erkennung (Position 4â€“15).

| Parameter | Typ | Standard | Beschreibung |
|-----------|-----|---------|--------------|
| `min_position` | `float` | `1.0` | Untergrenze Position-Filter |
| `max_position` | `float` | `20.0` | Obergrenze Position-Filter |

**Return-Format:** `list[dict]`
```python
[
    {
        "page": str,         # VollstÃ¤ndige URL
        "impressions": int,
        "clicks": int,
        "ctr": float,        # in %
        "position": float,   # Ã˜ Position, aufsteigend sortiert
    },
    ...
]
```

**API-Endpunkt:** `webmasters.searchanalytics.query` (Google Search Console API v3)
**Scope:** `https://www.googleapis.com/auth/webmasters.readonly`

---

### 3.3 RSS-Feed-Fetcher (`data/rss_reader.py`)

Ruft die in `config.RSS_FEEDS` definierten Feeds ab. Pro Feed werden max. `RSS_MAX_ITEMS_PER_FEED` (Standard: 15) Artikel geladen. Es existiert eine separate Funktion `fetch_google_news_articles(query)` fÃ¼r dynamische Google News-Suche nach einem Suchbegriff (genutzt in der Bewertungs-Pipeline).

**Return-Format pro Artikel:** `list[dict]`
```python
{
    "title": str,       # Artikel-Titel
    "url": str,         # Artikel-URL
    "source": str,      # Quellname (z.B. "NZZ Wirtschaft")
    "published": datetime | None,  # Publikationsdatum
    "summary": str,     # Kurzzusammenfassung / Teaser
}
```

---

### 3.4 Website-Crawler (`data/content_crawler.py`)

Lightweight-Crawler auf Basis von `requests` + stdlib `html.parser`. Kein Headless-Browser, kein JavaScript-Rendering.

**Funktion:** `crawl_top_pages(ga4_pages, base_url, limit)`

| Parameter | Typ | Standard | Beschreibung |
|-----------|-----|---------|--------------|
| `ga4_pages` | `list[dict]` | â€“ | GA4-Ergebnis-Liste (benÃ¶tigt `page_path`) |
| `base_url` | `str` | â€“ | Root-URL der Website |
| `limit` | `int` | `10` | Max. Anzahl zu crawlender Seiten |

**Return-Format:** `list[dict]`
```python
{
    "url": str,               # VollstÃ¤ndige URL
    "title": str,             # <title> oder <h1>
    "summary": str,           # Erste 4 SÃ¤tze des Textes
    "word_count": int,        # WÃ¶rteranzahl (sichtbarer Text)
    "estimated_date": str,    # Publikationsdatum (YYYY-MM-DD, best-effort)
    "error": str | None,      # Fehlermeldung bei Fehler
    "page_views": int,        # Aus GA4-Daten angereichert
    "engagement_rate": float, # Aus GA4-Daten angereichert
}
```

**Technische Details:**
- In-Memory-Cache: Jede URL wird pro Prozess-Laufzeit nur einmal gecrawlt
- Polite Crawl Delay: 0,3 Sekunden zwischen Requests
- Timeout: 8 Sekunden pro Request
- User-Agent: `Mozilla/5.0 (compatible; ContentIdeaBot/1.0; +https://github.com)`
- Datums-Extraktion: Meta-Tags (`article:published_time`, `datePublished`) + JSON-LD
- Ignorierte Tags: `script`, `style`, `nav`, `footer`, `header`, `aside`, `noscript`

**Formatierungsfunktion:** `format_crawl_summaries(crawled)` â†’ `str`
Wandelt die crawl-Ergebnisse in einen formatierten Markdown-Textblock fÃ¼r Agent-Prompts um.

---

### 3.5 Google Trends (`data/google_trends.py`)

Ruft via `pytrends` trendende Suchanfragen in der Schweiz (`geo="CH"`) ab. Die Konfiguration (`TRENDS_KEYWORDS`, `TRENDS_GEO`, `TRENDS_LIMIT`) liegt in `config.py`.

**Return-Format:** `list[dict]`
```python
[
    {"keyword": str, "value": int},  # value = relativer Trend-Index 0â€“100
    ...
]
```

Fehler (z.B. Rate-Limit) werden silent ignoriert und geben eine leere Liste zurÃ¼ck.

---

### 3.6 Artikel-Export (`export.py`)

Stellt zwei Ã¶ffentliche Funktionen fÃ¼r den Download-Export bereit.

**`article_to_markdown(article, social_snippets) â†’ str`**

Erzeugt einen Markdown-String mit folgender Struktur:
```
# {title}
_{lead}_
---
## {section.heading}
{section.content}
---
**Meta-Beschreibung:** {meta_description}
---
## Social Media
### LinkedIn / X / Twitter / Newsletter-Teaser
```

**`article_to_pdf(article, social_snippets) â†’ bytes`**

Rendert denselben Inhalt via `fpdf2` als PDF-Bytes:

| Element | Schrift | GrÃ¶ÃŸe |
|---------|---------|-------|
| Titel | Helvetica Bold | 20 pt |
| Lead | Helvetica Italic | 12 pt |
| Section-Heading | Helvetica Bold | 14 pt |
| Section-Content | Helvetica Regular | 11 pt |
| Meta-Beschreibung | Helvetica Italic, grau | 10 pt |
| Social Snippets | Neue Seite, Helvetica | 11â€“12 pt |

**`_slugify(text) â†’ str`**

Normalisiert den Artikel-Titel zu einem dateifreundlichen Slug (ASCII, Kleinbuchstaben, Bindestriche). Wird fÃ¼r den Dateinamen der Downloads verwendet.

**AbhÃ¤ngigkeit:** `fpdf2>=2.7.0` (Pure-Python, keine SystemabhÃ¤ngigkeiten wie LaTeX oder Ghostscript).

---

## 4. Pipeline-Orchestratoren

### 4.1 Ideen-Pipeline (`pipeline.py`)

**Einstiegspunkt:** `run(status_callback, token_callback) â†’ PipelineResult`

**Ablauf:**

```
Step 1: Parallel-Datenabruf (7 Threads, Timeout 45s gesamt)
        â”œâ”€â”€ GA4 (7 Tage)
        â”œâ”€â”€ GA4 (90 Tage)
        â”œâ”€â”€ GSC Queries (7 Tage)
        â”œâ”€â”€ GSC Queries (90 Tage)
        â”œâ”€â”€ GSC Seiten-Positionen (7 Tage)
        â”œâ”€â”€ RSS-Feeds
        â””â”€â”€ Google Trends (Schweiz)

Step 2: Website-Crawler (sequenziell, top 10 GA4-Seiten)

Step 3: Agent 1 â€“ Analyst
Step 4: Agent 2 â€“ Trend-Scout
Step 5: Agent 3 â€“ Stratege
Step 6: Agent 4 â€“ Redakteur

Step 7: Persistenz (ideas_history.json)
```

**ParallelitÃ¤t:** `concurrent.futures.ThreadPoolExecutor(max_workers=7)`. Alle 7 Datenabrufe laufen gleichzeitig. Deadline-basiertes Timeout: 45 Sekunden gesamt. Einzelne Fehler werden als Warnungen in `result.errors` gesammelt, stoppen aber nicht die Pipeline.

**Streaming:** Agenten 1â€“3 unterstÃ¼tzen Token-Streaming Ã¼ber `token_callback(phase, accumulated_text)`. Der UI-Bereich zeigt den laufenden Agent-Output in Echtzeit an (max. 800 Zeichen tail).

**Ideen-Persistenz:** `_save_ideas_history(ideas)` hÃ¤ngt nach jeder erfolgreichen Generierung einen Eintrag an `data/ideas_history.json` an. Retention: maximal 30 EintrÃ¤ge (Ã¤lteste werden gelÃ¶scht). Fehler bei der Persistenz werden still ignoriert (non-critical).

---

### 4.2 Artikel-Pipeline (`content_pipeline.py`)

**Einstiegspunkt:** `run(idea, rss_articles, gsc_queries, status_callback, target_words) â†’ ContentResult`

**Revision-Loop:**
```
Researcher (einmalig)
    â”‚
    â””â”€â–º Writer â†’ Fact-Checker â†’ Evaluator â”€â–º passed=True â†’ Social-Writer â†’ Ende
                       â–²              â”‚
                       â””â”€â”€ feedback â”€â”€â”˜ (passed=False, max. 2 Wiederholungen)
```

Max. DurchlÃ¤ufe: `MAX_REVISION_LOOPS + 1` (Standard: 3 DurchlÃ¤ufe total).
Social-Writer wird nur ausgefÃ¼hrt, wenn der Artikel die Evaluierung besteht.
Fehler in einzelnen Schritten werden in `result.errors` geloggt; der Writer-Fehler bricht die Loop ab, Fact-Checker- und Evaluator-Fehler fÃ¼hren zu Fallback-Werten.

---

### 4.3 Bewertungs-Pipeline (`evaluation_pipeline.py`)

**Einstiegspunkt:** `run(idea_title, idea_desc, rss_articles, ga4_pages, gsc_queries, status_callback) â†’ EvaluationResult`

**Ablauf:**
1. RSS-Abruf (falls nicht aus vorheriger Pipeline-AusfÃ¼hrung vorhanden)
2. Dynamische Google News-Suche nach Ideen-Titel
3. Agent: Kontext-Suche (findet relevante Datenpunkte)
4. Agent: Ideen-Bewertung (Urteil, Score, Pros/Cons)

**Datenstrategie:** Wenn `rss_articles`, `ga4_pages`, `gsc_queries` aus einer vorherigen Pipeline-AusfÃ¼hrung Ã¼bergeben werden, werden diese direkt genutzt (kein erneuter API-Aufruf). Nur RSS wird bei Bedarf frisch abgerufen.

---

## 5. Alle Agenten im Detail

### Ideen-Pipeline

#### Agent 1: Analyst (`agents/analyst.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Datengetriebener Content-Analyst |
| **Input** | GA4 7T, GA4 90T, GSC Queries 7T, GSC Queries 90T, GSC SeitenrÃ¤nge |
| **Output** | Strukturierter Markdown-Text mit Erkenntnissen |
| **Modell** | `OPENAI_MODEL` (gpt-5.2) |
| **Temperatur** | 0.3 |
| **Streaming** | Ja |

**Aufgaben laut Prompt:**
1. 3â€“5 performante Themenfelder (hohe Views + Engagement)
2. 3â€“5 Content-LÃ¼cken (hohe Impressionen, CTR < 3%)
3. Fast-Ranker (Seiten Position 4â€“15)
4. Evergreen vs. Kurzfrist-Trends (7T vs. 90T-Vergleich)
5. Ãœbergreifende Suchanfrage-Muster

---

#### Agent 2: Trend-Scout (`agents/trend_scout.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Wirtschaftsjournalist und Trend-Scout |
| **Input** | RSS-Artikel (max. 40) |
| **Output** | Strukturierter Markdown-Text: Top 10 Wirtschaftsthemen |
| **Modell** | `OPENAI_MODEL` (gpt-5.2) |
| **Temperatur** | 0.3 |
| **Streaming** | Ja |

**Aufgaben laut Prompt:**
1. Top-10-Themen filtern und gruppieren
2. FÃ¼r jedes Thema: Kurztitel, Relevanz-BegrÃ¼ndung, Quelle

---

#### Agent 3: Stratege (`agents/strategist.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Content-Stratege |
| **Input** | Analyst-Output (str), Trend-Scout-Output (str), Crawl-Summaries (str) |
| **Output** | `IDEAS_COUNT` Ideen-Konzepte als strukturierter Text |
| **Modell** | `OPENAI_MODEL` (gpt-5.2) |
| **Temperatur** | 0.5 |
| **Streaming** | Ja |

Pro Idee im Output:
- Thema/Konzept (kein fertiger Titel)
- Kernbotschaft
- Daten-Basis (Signale)
- Typ: Neuer Artikel oder `[UPDATE]`

Wenn Crawl-Summaries vorhanden: Duplikate werden explizit vermieden, Update-Kandidaten markiert.

---

#### Agent 4: Redakteur (`agents/editor.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Erfahrener digitaler Redakteur |
| **Input** | Strategen-Output (str), RSS-Artikel (list, fÃ¼r Link-Referenzen) |
| **Output** | JSON: `{"ideas": [...]}` |
| **Modell** | `OPENAI_MODEL` (gpt-5.2) |
| **Temperatur** | 0.6 |
| **Response-Format** | `json_object` |
| **Streaming** | Nein |

**Output-Schema pro Idee:**
```json
{
  "title": "string (10-15 WÃ¶rter)",
  "why_now": "string (2-3 SÃ¤tze)",
  "category": "string (z.B. 'Geldpolitik')",
  "signals": {
    "ga4": "string (Beobachtung â†’ BegrÃ¼ndung)",
    "gsc": "string (Beobachtung â†’ BegrÃ¼ndung)",
    "rss": "string (Beobachtung â†’ BegrÃ¼ndung)"
  },
  "rss_links": [
    {"title": "string", "url": "string", "source": "string"}
  ],
  "score": "A|B|C"  // nach-hoc berechnet (nicht im Prompt)
}
```

**Score-Berechnung (post-processing):** Nach Erhalt der LLM-Antwort berechnet `_compute_score()` den Score anhand der befÃ¼llten Signal-Felder. Anschliessend werden die Ideen nach Score sortiert (A â†’ B â†’ C).

---

### Artikel-Pipeline

#### Agent 5: Researcher (`agents/researcher.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | PrÃ¤ziser Rechercheur |
| **Input** | `idea` (dict), `rss_articles` (list), `gsc_queries` (list) |
| **Output** | Strukturierter Markdown-Text (Research-Notes) |
| **Modell** | `OPENAI_MODEL_PRO` (gpt-5.2) |
| **Temperatur** | 0.2 |

**Output-Abschnitte:** Kernthesen Â· Belege & Datenpunkte Â· Leserfragen (aus GSC) Â· Empfohlene Artikel-Struktur Â· Relevante Quellen

Kein externer API-Aufruf â€“ arbeitet ausschliesslich mit den Ã¼bergebenen Daten.

---

#### Agent 6: Writer (`agents/writer.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Wirtschaftsjournalist |
| **Input** | `idea`, `research_notes`, `brand_voice`, `forbidden_phrases`, `target_words`, `revision_feedback?` |
| **Output** | JSON-Artikel-Objekt |
| **Modell** | `OPENAI_MODEL_PRO` (gpt-5.2) |
| **Temperatur** | 0.7 |
| **Response-Format** | `json_object` |

**Output-Schema:**
```json
{
  "title": "string",
  "lead": "string (2-3 SÃ¤tze)",
  "sections": [
    {"heading": "string", "content": "string (Markdown: Fett, Listen, AbsÃ¤tze)"}
  ],
  "meta_description": "string (max. 160 Zeichen)"
}
```

Bei `revision_feedback` (Folgedurchlauf): Feedback wird explizit in den Prompt integriert.
Wort-Ziel: Â±15% Toleranz um `target_words`.

---

#### Agent 7: Fact-Checker (`agents/fact_checker.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Strenger Fact-Checker |
| **Input** | `article` (dict), `research_notes` (str) |
| **Output** | Korrigiertes Artikel-JSON (gleiche Struktur) |
| **Modell** | `OPENAI_MODEL_PRO` (gpt-5.2) |
| **Temperatur** | 0.1 |
| **Response-Format** | `json_object` |

Research-Notes sind die einzige Wahrheitsquelle. Unbelegte Fakten werden entfernt oder korrigiert. Stil und Struktur bleiben erhalten. Bei fehlenden Keys: Fallback auf Original-Artikel-Werte.

---

#### Agent 8: Evaluator (`agents/evaluator.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Kritischer Chefredakteur |
| **Input** | `article` (dict), `idea` (dict) |
| **Output** | JSON-Bewertungsobjekt |
| **Modell** | `OPENAI_MODEL_PRO` (gpt-5.2) |
| **Temperatur** | 0.3 |
| **Response-Format** | `json_object` |

**Output-Schema:**
```json
{
  "scores": {
    "authentizitaet": 0-100,
    "tiefe": 0-100,
    "klarheit": 0-100,
    "relevanz": 0-100
  },
  "overall": 0-100,     // Ã˜ der 4 Scores (server-seitig neu berechnet zur Sicherheit)
  "passed": true|false, // overall >= EVALUATOR_MIN_SCORE (80)
  "feedback": "string"  // Leer wenn passed=true
}
```

Post-Processing: `overall` und `passed` werden client-seitig neu berechnet (LLM-Antwort als Datenbasis, aber eigene Berechnung ist autoritativ).

---

#### Agent 9: Social-Writer (`agents/social_writer.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Social-Media-Redakteur |
| **Input** | `article` (dict), `idea_title` (str) |
| **Output** | JSON mit 3 Plattform-Texten |
| **Modell** | `OPENAI_MODEL_PRO` (gpt-5.2) |
| **Temperatur** | 0.7 |
| **Response-Format** | `json_object` |

Nur die ersten 3 Artikel-Abschnitte werden im Prompt genutzt (Token-Effizienz).

**Output-Schema:**
```json
{
  "linkedin": "string (~1200 Zeichen, professionell, endet mit Frage/CTA)",
  "twitter": "string (max. 280 Zeichen, max. 1-2 Hashtags)",
  "newsletter_teaser": "string (2 SÃ¤tze)"
}
```

---

### Bewertungs-Pipeline

#### Agent 10: Kontext-Agent (`agents/idea_context.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Daten-Rechercheur |
| **Input** | `idea_title`, `idea_desc`, `rss_articles`, `ga4_pages`, `gsc_queries` |
| **Output** | Strukturierter Markdown-Text |
| **Modell** | `OPENAI_MODEL` (gpt-5.2) |
| **Temperatur** | 0.3 |

**Output-Abschnitte:** Relevante RSS-Artikel Â· GA4-Signale Â· GSC-Signale Â· Fazit (Datenlage-Bewertung)

---

#### Agent 11: Bewertungs-Agent (`agents/idea_evaluator.py`)
| Attribut | Wert |
|----------|------|
| **Rolle** | Erfahrener Content-Stratege |
| **Input** | `idea_title`, `idea_desc`, `context` (Kontext-Agent-Output) |
| **Output** | JSON-Bewertungsobjekt |
| **Modell** | `OPENAI_MODEL` (gpt-5.2) |
| **Temperatur** | 0.4 |
| **Response-Format** | `json_object` |

**Output-Schema:**
```json
{
  "verdict": "Empfohlen|Mit Vorbehalt|Nicht empfohlen",
  "score": 0-100,
  "pros": ["string", ...],
  "cons": ["string", ...],
  "recommendation": "string"
}
```

---

## 6. Datenmodelle

### `PipelineResult` (`pipeline.py`)
```python
@dataclass
class PipelineResult:
    ideas: list[dict]          # Fertige Ideen (Editor-Output)
    analyst_output: str        # Rohtext Agent 1
    trend_scout_output: str    # Rohtext Agent 2
    strategist_output: str     # Rohtext Agent 3
    errors: list[str]          # Gesammelte Fehler/Warnungen
    ga4_pages: list[dict]      # GA4 7-Tage-Daten
    gsc_queries: list[dict]    # GSC Queries 7-Tage
    rss_articles: list[dict]   # RSS-Artikel
    gsc_pages: list[dict]      # GSC SeitenrÃ¤nge
    ga4_pages_long: list[dict] # GA4 90-Tage-Daten
    gsc_queries_long: list[dict] # GSC Queries 90-Tage
    crawled_pages: list[dict]  # Website-Crawler-Ergebnisse
    fetched_at: datetime | None
```

### `ContentResult` (`content_pipeline.py`)
```python
@dataclass
class ContentResult:
    article: dict          # Fertiger Artikel (Writer-Output, fact-checked)
    evaluation: dict       # Evaluator-Output mit Scores
    research_notes: str    # Researcher-Output (Rohtext)
    revision_count: int    # Anzahl Ãœberarbeitungsrunden (0 = kein Revision)
    social_snippets: dict  # Social-Writer-Output
    errors: list[str]
```

### `EvaluationResult` (`evaluation_pipeline.py`)
```python
@dataclass
class EvaluationResult:
    verdict: str       # "Empfohlen" | "Mit Vorbehalt" | "Nicht empfohlen"
    score: int         # 0â€“100
    pros: list[str]
    cons: list[str]
    recommendation: str
    context_notes: str  # Kontext-Agent-Rohtext (fÃ¼r show_details)
    errors: list[str]
```

---

## 7. Konfiguration (`config.py`)

| Konstante | Typ | Standard | Beschreibung |
|-----------|-----|---------|--------------|
| `RSS_FEEDS` | `list[dict]` | 4 Feeds | Definierte RSS-Quellen (`name`, `url`) |
| `ANALYTICS_DAYS_BACK` | `int` | `7` | Kurzfristiger GA4/GSC-Zeitraum |
| `ANALYTICS_DAYS_LONG` | `int` | `90` | Langfristiger Zeitraum (Evergreen-Vergleich) |
| `CRAWL_TOP_PAGES` | `int` | `10` | Anzahl GA4-Seiten fÃ¼r Website-Crawler |
| `IDEAS_COUNT` | `int` | `5` | Anzahl zu generierender Ideen |
| `TRENDS_GEO` | `str` | `"CH"` | Region fÃ¼r Google Trends (ISO-3166-Alpha-2) |
| `TRENDS_LIMIT` | `int` | `20` | Anzahl trendender Keywords |
| `RSS_MAX_ITEMS_PER_FEED` | `int` | `15` | Maximale RSS-Artikel pro Feed |
| `OPENAI_MODEL` | `str` | `"gpt-5.2"` | Modell fÃ¼r Ideen-Pipeline-Agenten |
| `OPENAI_MODEL_PRO` | `str` | `"gpt-5.2"` | Modell fÃ¼r Artikel-Pipeline-Agenten |
| `ARTICLE_TARGET_WORDS` | `int` | `1200` | Standard-Zielwortanzahl fÃ¼r Artikel |
| `MAX_REVISION_LOOPS` | `int` | `2` | Max. Ãœberarbeitungsrunden nach Erstdraft |
| `EVALUATOR_MIN_SCORE` | `int` | `80` | Mindest-Score fÃ¼r bestandene Bewertung |
| `BRAND_VOICE` | `str` | â€“ | Schreibstil-Anweisung (mehrzeilig) |
| `FORBIDDEN_PHRASES` | `list[str]` | 10 Phrasen | Verbotene Formulierungen fÃ¼r Writer-Agent |

**Aktuelle `BRAND_VOICE`:**
```
Sachlich, direkt und faktenbasiert. Komplexe Wirtschaftsthemen
verstÃ¤ndlich und ohne Jargon erklÃ¤ren. Schweizer Perspektive
wo relevant. Keine werbliche Sprache, kein Eigenlob.
```

---

## 8. Umgebungsvariablen

| Variable | Pflicht | Beschreibung |
|----------|---------|--------------|
| `OPENAI_API_KEY` | Ja | OpenAI API-Key |
| `GA4_PROPERTY_ID` | Nein* | Google Analytics 4 Property-ID (reine Zahl) |
| `GSC_SITE_URL` | Nein* | Google Search Console Site-URL (vollstÃ¤ndige URL inkl. Protokoll) |
| `GOOGLE_CREDENTIALS_FILE` | Nein** | Pfad zur Service-Account-JSON-Datei (Standard: `"credentials.json"`) |
| `GOOGLE_CREDENTIALS_JSON` | Nein** | Inhalt der Service-Account-JSON-Datei (fÃ¼r Streamlit Cloud / Secrets) |

\* Ohne GA4/GSC-Credentials lÃ¤uft die App, jedoch ohne Google-Daten. RSS-basierte Ideen werden trotzdem generiert.
\*\* Eine der beiden Google-Credentials-Varianten ist erforderlich, wenn GA4/GSC-Daten genutzt werden sollen.

**Streamlit Cloud:** `st.secrets` wird beim App-Start in `os.environ` Ã¼berfÃ¼hrt. TOML-Sektionen werden als JSON-String serialisiert. Damit kÃ¶nnen Credentials direkt als Secret hinterlegt werden.

---

## 9. Ideen-Scoring

### Algorithmus (`agents/editor.py â†’ _compute_score`)

```python
def _compute_score(signals: dict) -> str:
    count = sum(
        1 for v in (signals.get("ga4"), signals.get("gsc"), signals.get("rss"))
        if v  # Nicht-leerer String = Signal vorhanden
    )
    if count >= 3:
        return "A"
    if count == 2:
        return "B"
    return "C"
```

| Score | Bedingung | Bedeutung |
|-------|-----------|-----------|
| `A` | Alle 3 Signale befÃ¼llt | HÃ¶chste Datenkonfidenz |
| `B` | Genau 2 Signale befÃ¼llt | Gute Datenkonfidenz |
| `C` | 0 oder 1 Signal befÃ¼llt | Niedrige Datenkonfidenz |

### Sortierung
```python
score_order = {"A": 0, "B": 1, "C": 2}
result.sort(key=lambda x: score_order.get(x.get("score", "C"), 2))
```

A-Ideen erscheinen in der UI immer zuerst, C-Ideen zuletzt.

---

## 10. Ideen-Persistenz

**Datei:** `data/ideas_history.json`
**Format:** JSON-Array mit maximal 30 EintrÃ¤gen

```json
[
  {
    "generated_at": "2026-02-26T14:30:00.123456",
    "ideas": [
      {
        "title": "string",
        "why_now": "string",
        "category": "string",
        "signals": {"ga4": "string", "gsc": "string", "rss": "string"},
        "rss_links": [{"title": "string", "url": "string", "source": "string"}],
        "score": "A|B|C"
      }
    ]
  },
  ...
]
```

**Retention-Policy:** Bei jedem Schreiben wird `history[-30:]` genommen â€“ Ã¤lteste EintrÃ¤ge werden automatisch verworfen.

**Fehlerverhalten:** `_save_ideas_history` ist in einem `try/except` gewrappt und silent-fails â€“ ein Persistenz-Fehler unterbricht nie die Pipeline.

**Laden:** `load_ideas_history() â†’ list[dict]` liest die Datei direkt aus. Bei fehlender Datei oder Parse-Fehler: leere Liste.

**UI:** Die Sidebar zeigt die letzten 5 Generierungen (neueste zuerst) mit Zeitstempel, Ideen-Anzahl und Score-Badges.

---

## 11. VollstÃ¤ndiges Data-Flow-Diagramm

```
                    UMGEBUNGSVARIABLEN
                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         OPENAI_API_KEY  GA4_PROPERTY_ID  GSC_SITE_URL
         GOOGLE_CREDENTIALS_FILE / GOOGLE_CREDENTIALS_JSON
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE.RUN()                           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Parallel-Datenabruf (ThreadPoolExecutor, 7 Threads) â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚  google_analytics.py          rss_reader.py          â”‚  â”‚
â”‚  â”‚  fetch_top_pages(7T)  â”€â”€â”  â”Œâ”€â”€ fetch_rss_articles()  â”‚  â”‚
â”‚  â”‚  fetch_top_pages(90T) â”€â”€â”¤  â”‚                         â”‚  â”‚
â”‚  â”‚                         â”‚  â”‚  search_console.py      â”‚  â”‚
â”‚  â”‚  fetch_top_queries(7T) â”€â”¤  â”œâ”€â”€ fetch_top_queries(7T) â”‚  â”‚
â”‚  â”‚  fetch_top_queries(90T)â”€â”¤  â”œâ”€â”€ fetch_top_queries(90T)â”‚  â”‚
â”‚  â”‚  fetch_top_pages_by_posâ”€â”¤  â””â”€â”€ (gsc pages)           â”‚  â”‚
â”‚  â”‚                         â”‚                            â”‚  â”‚
â”‚  â”‚  google_trends.py       â”‚                            â”‚  â”‚
â”‚  â”‚  fetch_trending_topicsâ”€â”€â”˜                            â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚  Return: list[dict] je Quelle    Timeout: 45s        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  content_crawler.py                                  â”‚  â”‚
â”‚  â”‚  crawl_top_pages(ga4_pages, base_url, limit=10)      â”‚  â”‚
â”‚  â”‚  â†’ list[dict]: {url, title, summary, word_count,     â”‚  â”‚
â”‚  â”‚                 estimated_date, page_views, error}   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  agents/analyst.py  (temp=0.3, streaming)            â”‚  â”‚
â”‚  â”‚  Input:  ga4_7T, gsc_queries_7T, gsc_pages,          â”‚  â”‚
â”‚  â”‚          ga4_90T, gsc_queries_90T                    â”‚  â”‚
â”‚  â”‚  Output: str (Markdown, Analyst-Erkenntnisse)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  agents/trend_scout.py  (temp=0.3, streaming)        â”‚  â”‚
â”‚  â”‚  Input:  rss_articles (max. 40)                      â”‚  â”‚
â”‚  â”‚  Output: str (Markdown, Top-10-Themen)               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  agents/strategist.py  (temp=0.5, streaming)         â”‚  â”‚
â”‚  â”‚  Input:  analyst_output, trend_scout_output,         â”‚  â”‚
â”‚  â”‚          crawl_summaries (formatted str)             â”‚  â”‚
â”‚  â”‚  Output: str (Markdown, IDEAS_COUNT Ideen-Konzepte)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  agents/editor.py  (temp=0.6, json_object)           â”‚  â”‚
â”‚  â”‚  Input:  strategist_output, rss_articles (max. 30)   â”‚  â”‚
â”‚  â”‚  Output: list[dict] (Ideen mit title/why_now/        â”‚  â”‚
â”‚  â”‚          category/signals/rss_links/score)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚              _save_ideas_history()                          â”‚
â”‚              â†’ data/ideas_history.json (max. 30 Runs)      â”‚
â”‚                                                             â”‚
â”‚  Return: PipelineResult                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ (Benutzer wÃ¤hlt Idee)
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CONTENT_PIPELINE.RUN()                     â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  agents/researcher.py  (temp=0.2)                    â”‚  â”‚
â”‚  â”‚  Input:  idea, rss_articles, gsc_queries             â”‚  â”‚
â”‚  â”‚  Output: str (Research-Notes: Markdown)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  REVISION-LOOP  (max. MAX_REVISION_LOOPS+1 = 3x)     â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚  agents/writer.py  (temp=0.7, json_object)           â”‚  â”‚
â”‚  â”‚  Input:  idea, research_notes, brand_voice,          â”‚  â”‚
â”‚  â”‚          forbidden_phrases, target_words,            â”‚  â”‚
â”‚  â”‚          revision_feedback? (str|None)               â”‚  â”‚
â”‚  â”‚  Output: dict {title, lead, sections, meta_desc}     â”‚  â”‚
â”‚  â”‚               â”‚                                      â”‚  â”‚
â”‚  â”‚  agents/fact_checker.py  (temp=0.1, json_object)     â”‚  â”‚
â”‚  â”‚  Input:  article, research_notes                     â”‚  â”‚
â”‚  â”‚  Output: dict (korrigiertes Artikel-JSON)            â”‚  â”‚
â”‚  â”‚               â”‚                                      â”‚  â”‚
â”‚  â”‚  agents/evaluator.py  (temp=0.3, json_object)        â”‚  â”‚
â”‚  â”‚  Input:  article, idea                               â”‚  â”‚
â”‚  â”‚  Output: dict {scores, overall, passed, feedback}    â”‚  â”‚
â”‚  â”‚               â”‚                                      â”‚  â”‚
â”‚  â”‚          passed=True? â”€â”€â”€ Ja â†’ Exit Loop             â”‚  â”‚
â”‚  â”‚               â”‚                                      â”‚  â”‚
â”‚  â”‚          Nein: revision_feedback â†’ Writer (nÃ¤chster  â”‚  â”‚
â”‚  â”‚          Durchlauf) bis max. Loops erreicht          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚ (nur wenn passed=True)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  agents/social_writer.py  (temp=0.7, json_object)    â”‚  â”‚
â”‚  â”‚  Input:  article (sections[:3]), idea_title          â”‚  â”‚
â”‚  â”‚  Output: dict {linkedin, twitter, newsletter_teaser} â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  Return: ContentResult                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ (Artikel in app.py dargestellt)
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXPORT (export.py)                     â”‚
â”‚                                                             â”‚
â”‚  article_to_markdown(article, social_snippets) â†’ str        â”‚
â”‚  article_to_pdf(article, social_snippets) â†’ bytes (fpdf2)   â”‚
â”‚  _slugify(title) â†’ str  (fÃ¼r Dateinamen)                    â”‚
â”‚                                                             â”‚
â”‚  UI: st.download_button "ğŸ“„ Markdown" + "ğŸ–¨ï¸ PDF"            â”‚
â”‚      st.button "ğŸŒ CMS importieren" (disabled)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                  (parallel, unabhÃ¤ngig von Artikel-Pipeline)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                EVALUATION_PIPELINE.RUN()                    â”‚
â”‚                                                             â”‚
â”‚  RSS-Fetch (falls kein Cache) + Google News-Suche           â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  agents/idea_context.py  (temp=0.3)                  â”‚  â”‚
â”‚  â”‚  Input:  idea_title, idea_desc, rss, ga4, gsc        â”‚  â”‚
â”‚  â”‚  Output: str (Kontext-Markdown)                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  agents/idea_evaluator.py  (temp=0.4, json_object)   â”‚  â”‚
â”‚  â”‚  Input:  idea_title, idea_desc, context              â”‚  â”‚
â”‚  â”‚  Output: dict {verdict, score, pros, cons,           â”‚  â”‚
â”‚  â”‚                recommendation}                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                â”‚
â”‚  Return: EvaluationResult                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*Stand: Februar 2026 â€“ zuletzt aktualisiert: Artikel-Export (MD/PDF), Google Trends, 7-Thread-ParallelitÃ¤t*
